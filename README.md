# MELD-Eval

A Fine-Grained Evaluation Framework for Language Models: Combining Pointwise Grading and Pairwise Comparison


### ðŸŽ¬ a demo

https://github.com/user-attachments/assets/dac18c34-6b96-4bd0-aee0-6faba6bc1b9a




## Directory Structure

```
MELD-Eval/
â”œâ”€â”€ README.md                      # Project overview and usage instructions
â”œâ”€â”€ requirement.txt               # Python dependencies required for the project
â”œâ”€â”€ data/                         # All datasets used for training and evaluation
â”‚   â”œâ”€â”€ test/                     # Evaluation datasets
â”‚   â”‚   â”œâ”€â”€ bias/                 # For bias detection (position, content, length, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ LLMBar/           # Five sub-datasets for bias testing
â”‚   â”‚   â”‚   â””â”€â”€ MELD_length_bias_test.json  # Test set for analyzing length bias
â”‚   â”‚   â”œâ”€â”€ pairwise/             # Datasets for pairwise comparison evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ MELD-Test.json
â”‚   â”‚   â”‚   â””â”€â”€ PandaLM-Test.json
â”‚   â”‚   â””â”€â”€ pointwise/            # Datasets for pointwise scoring evaluation
â”‚   â”‚       â”œâ”€â”€ Auto-J-Test.json
â”‚   â”‚       â”œâ”€â”€ Feedback-Bech.json
â”‚   â”‚       â””â”€â”€ MELD-Test.json
â”‚   â”œâ”€â”€ train/                    # General-purpose training sets (raw format)
â”‚   â”‚   â”œâ”€â”€ pairwise.json
â”‚   â”‚   â””â”€â”€ pointwise.json
â”‚   â””â”€â”€ train_for_llama_factory/ # Training sets formatted for LLaMA-Factory
â”‚       â”œâ”€â”€ pairwise.json
â”‚       â””â”€â”€ pointwise.json
â”œâ”€â”€ model/                        # Model training and merging configs/scripts
â”‚   â”œâ”€â”€ merge/                    # Configuration files for model merging strategies
â”‚   â”‚   â”œâ”€â”€ dare.yaml
â”‚   â”‚   â”œâ”€â”€ linear.yaml
â”‚   â”‚   â”œâ”€â”€ slerp.yaml
â”‚   â”‚   â””â”€â”€ ties.yaml
â”‚   â””â”€â”€ train/                    # Shell scripts to train individual and merged models
â”‚       â”œâ”€â”€ train_merge.sh
â”‚       â”œâ”€â”€ train_pairwise.sh
â”‚       â””â”€â”€ train_pointwise.sh
â”œâ”€â”€ src/                          # Source code for evaluation and inference
â”‚   â”œâ”€â”€ criteria/                 # Evaluation criteria definitions for 8 major task categories
â”‚   â”‚   â”œâ”€â”€ casual conversation.txt
â”‚   â”‚   â”œâ”€â”€ coding.txt
â”‚   â”‚   â”œâ”€â”€ math.txt
â”‚   â”‚   â”œâ”€â”€ nlp task.txt
â”‚   â”‚   â”œâ”€â”€ professional knowledge.txt
â”‚   â”‚   â”œâ”€â”€ reasoning.txt
â”‚   â”‚   â”œâ”€â”€ roleplay.txt
â”‚   â”‚   â””â”€â”€ writing.txt
â”‚   â”œâ”€â”€ eval/                     # Scripts for evaluating model predictions
â”‚   â”‚   â”œâ”€â”€ pairwise_eval.py
â”‚   â”‚   â””â”€â”€ pointwise_eval.py
â”‚   â””â”€â”€ infer/                    # Inference scripts for generating model outputs
â”‚       â”œâ”€â”€ pairwise_infer.py
â”‚       â””â”€â”€ pointwise_infer.py
```

## Directory Structure for Models

To help clarify the model directory structure used in this project, here's an overview of the model directories and their purposes:
```
MELD-Eval/models/
â”œâ”€â”€ eval_model/                         # Base models and baseline models
â”‚   â”œâ”€â”€ Meta-Llama-3-8B-Instruct/       # Base LLaMA-3 model
â”‚   â”œâ”€â”€ autoj-13b/                      # Auto-J baseline model
â”‚   â”œâ”€â”€ prometheus-7b-v2.0/             # Prometheus baseline model
â”‚   â””â”€â”€ PandaLM-7B-v1/                  # PandaLM baseline model
â”œâ”€â”€ MELD/                               # MELD model-related files
â”‚   â”œâ”€â”€ lora/                           # LoRA weights from training
â”‚   â”‚   â””â”€â”€ sft/
â”‚   â”‚       â”œâ”€â”€ pointwise/              # LoRA weights for pointwise grading
â”‚   â”‚       â””â”€â”€ pairwise/               # LoRA weights for pairwise comparison
â”‚   â”œâ”€â”€ pointwise_model/                # Complete pointwise model (after LoRA merging)
â”‚   â”œâ”€â”€ pairwise_model/                 # Complete pairwise model (after LoRA merging)
â”‚   â”œâ”€â”€ MELD-8B/                        # Final unified MELD model (after MergeKit)
â”‚   â””â”€â”€ Q4_K-dare-merge-judge-llama-3-8b-instruct-gguf  # Quantized model
```
**Note**: These models need to be either trained from the base model or downloaded from external sources:
- Base models (in `eval_model/`): Download from Hugging Face or other model repositories
- LoRA weights (in `MELD/lora/`): Generated through training with LLaMA-Factory
- Complete single-task models (in `pointwise_model/` and `pairwise_model/`): Created by merging LoRA weights with the base model
- Final MELD model (in `MELD-8B/`): Created by merging the single-task models
- Quantized model: Generated through llama.cpp quantization process

The detailed process for obtaining or training each model is described in the following sections.

## Model Training Pipeline

The MELD model is trained using the [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) framework. The process includes the following main steps: environment setup, data preparation, model training, and LoRA weight merging. The full workflow is provided below for researchers who wish to reproduce our results.

---

1. **Environment Setup**
    
    First, clone the LLaMA-Factory repository and configure the environment:
    
    ```bash
    # Clone the LLaMA-Factory repository
    cd ./MELD-Eval
    git clone https://github.com/hiyouga/LLaMA-Factory.git
    # Create and activate the conda environment
    conda create -n llama_factory python=3.12
    conda activate llama_factory
    # Install LLaMA-Factory
    cd LLaMA-Factory
    pip install -e '.[torch,metrics]'
    ```

---

2. **Download the Base Model**
    
    Download the LLaMA-3-8B-Instruct model from Hugging Face:
    
    ```bash
    mkdir ./MELD-Eval/models/eval_model
    cd ./MELD-Eval/models/eval_model
    git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
    ```

---

3. **Data Preparation**
    
    We provide two key training datasets in this repository under the `./MELD-Eval/data/train_for_llama_factory/` directory:
    
    * `pointwise.json`: for pointwise scoring model training
    * `pairwise.json`: for pairwise comparison model training
    
    Copy these files to the `data/` directory of LLaMA-Factory:
    
    ```bash
    cp ./MELD-Eval/data/train_for_llama_factory/pointwise.json ./MELD-Eval/LLaMA-Factory/data/
    cp ./MELD-Eval/data/train_for_llama_factory/pairwise.json ./MELD-Eval/LLaMA-Factory/data/
    ```
    
    Then register the datasets in LLaMA-Factory by editing `./MELD-Eval/LLaMA-Factory/data/dataset_info.json` and adding the following entries:
    
    ```json
    {
      "MELD_pairwise": {
        "file_name": "pairwise.json",
        "columns": {
          "prompt": "instruction",
          "query": "input",
          "response": "output"
        }
      },
      "MELD_pointwise": {
        "file_name": "pointwise.json",
        "columns": {
          "prompt": "instruction",
          "query": "input",
          "response": "output"
        }
      }
    }
    ```

---

4. **Train the Pointwise Grading and Pairwise Comparison Model**
    
    ```bash
    # Run the training script
    bash ./MELD-Eval/models/train/train_pointwise.sh
     # Run the training script
    bash ./MELD-Eval/models/train/train_pairwise.sh
    ```

---

5. **Merge LoRA Weights into the Base Model (Step 1 of Model Merging)**

    First, we need to merge the LoRA weights back into the base model to create complete fine-tuned models for both pointwise grading and pairwise comparison.
    ```bash
    # Run the merging script
    bash ./MELD-Eval/models/train/train_merge.sh
    ```
    This script will create two separate models:
    - Pointwise grading model at `./MELD-Eval/models/MELD/pointwise_model`
    - Pairwise comparison model at `./MELD-Eval/models/MELD/pairwise_model`

## Model Merging (Step 2 of Model Merging)

After obtaining the complete fine-tuned pointwise grading model and pairwise comparison model from Step 1, we use the [MergeKit](https://github.com/arcee-ai/mergekit) to perform model merging, creating the final MELD evaluation model through different merging strategies.

---

1. **Installing MergeKit**

    First, install the MergeKit toolï¼š

   ```bash
    pip install mergekit
   ```

---

2. **Preparing Merge Configuration Files**

    We provide various merging strategy configuration files in the `./MELD-Eval/models/merge` directory, including:

    * `dare.yaml` - Configuration based on DARE (Drop And Rescale) strategy.
    * `linear.yaml` - Linear weighted merging configuration.
    * `slerp.yaml` - Spherical linear interpolation merging configuration.
    * `ties.yaml` - Configuration based on TIES method.

    According to our experimental results, the DARE strategy provides the best performance while maintaining both evaluation capabilities.

---

3. **Executing Model Merging**

    Use the following command to execute model merging:

    ```bash
    # Using DARE strategy to merge models
    mergekit-yaml ./MELD-Eval/models/merge/dare.yaml --out ./MELD-Eval/models/MELD/MELD-8B

    # Or try other merging strategies
    # mergekit-yaml ./MELD-Eval/models/merge/linear.yaml --out ./MELD-Eval/models/MELD-8B-linear
    # mergekit-yaml ./MELD-Eval/models/merge/slerp.yaml --out ./MELD-Eval/models/MELD-8B-slerp
    # mergekit-yaml ./MELD-Eval/models/merge/ties.yaml --out ./MELD-Eval/models/MELD-8B-ties
    ```

After merging is complete, the final MELD model will be saved in the `./MELD-Eval/models/MELD/MELD-8B` directory.

## Model quantization

Next, we'll quantize the merged model using [llama.cpp](https://github.com/ggml-org/llama.cpp).

---

1. **Environment Setup**
   
   First, install CMake:
   - For Linux:
    ```bash
     sudo apt update
     sudo apt install cmake
    ```
   - For Mac:
    ```bash
     brew install cmake
    ```
   Then, setting up llama.cpp Environment:
   ```bash
   # Create and Activate a Virtual Environment
   conda create -n llamacpp python=3.10 -y
   conda activate llamacpp
   ```
   ```bash
   # Install llama.cpp
   cd ./MELD-Eval
   git clone https://github.com/ggerganov/llama.cpp.git
   cd llama.cpp
   ```
   ```bash
   #Install Dependencies
   pip install -r requirements.txt
   ```
   ```bash
   #Compile the llama.cpp Project
   mkdir build
   cd build
   cmake ..
   cmake --build . --config Release
   ```
---

2. **Convert the Model to GGUF Format**

   Next, we will convert the model stored in `./MELD-Eval/models/MELD/MELD-8B` to the GGUF format.
   ```bash
   python ./MELD-Eval/llama.cpp/convert_hf_to_gguf.py ./MELD-Eval/models/MELD/MELD-8B --outfile ./MELD-Eval/models/MELD/dare-merge-judge-llama-3-8b-instruct-gguf
   ```
---

3. **Quantize the Model**

   Use the following command to execute model quantizing:

   ```bash
   ./MELD-Eval/llama.cpp/build/bin/llama-quantize ./MELD-Eval/models/MELD/dare-merge-judge-llama-3-8b-instruct-gguf ./MELD-Eval/models/MELD/Q4_K-dare-merge-judge-llama-3-8b-instruct-gguf Q4_K
   ```

After 4-bit quantization, the MELD-8B model will be saved in the `./MELD-Eval/models/MELD/Q4_K-dare-merge-judge-llama-3-8b-instruct-gguf`

## Model Inference and Result Evaluation
---

1. **Model Setup**
    
    Before running the evaluation framework, you need to download and set up the required models. The framework uses several models including MELD-8B, LLaMA-3-8B-Instruct, [PandaLM-7B](https://github.com/WeOpenML/PandaLM), [Auto-J-13B](https://github.com/GAIR-NLP/auto-j), and [Prometheus-7B](https://github.com/prometheus-eval/prometheus-eval). After downloading, models should be saved to the following paths:
    
    ```
    MELD: ./MELD-Eval/models/MELD/MELD-8B/
    Llama3: ./MELD-Eval/models/eval_model/Meta-Llama-3-8B-Instruct
    AutoJ: ./MELD-Eval/models/eval_model/autoj-13b/
    Prometheus: ./MELD-Eval/models/eval_model/prometheus-7b-v2.0
    PandaLM: ./MELD-Eval/models/eval_model/PandaLM-7B-v1/
    ```

---

2. **Model Inference**
    
    - Pointwise Grading
    
    ```bash
    python ./MELD-Eval/src/infer/pointwise_infer.py --model meld --input_file ./MELD-Eval/data/test/pointwise/MELD-Test.json --output_dir ./MELD-Eval/src/results/pointwise/ --criteria_dir ./MELD-Eval/src/criteria/
    ```
   Input Format
    ```json
    [
      {
        "id": 1,
        "question_body": "Question text",
        "answer_body": "Response to evaluate",
        "label": "Optional human reference score",  
        "category": "Category" 
      }
    ]
    ```
    
    
    * Pariwise Comparison
    
    ```bash
    # Using original response order
    python ./MELD-Eval/src/infer/pairwise_infer.py --model meld --input_file ./MELD-Eval/data/test/pairwise/MELD-Test.json --output_dir ./MELD-Eval/src/results/pairwise/ --response_order original --criteria_dir ./MELD-Eval/src/criteria/

    # Using swapped response order (for position bias analysis)
    python ./MELD-Eval/src/infer/pairwise_infer.py --model meld --input_file ./MELD-Eval/data/test/pairwise/MELD-Test.json --output_dir ./MELD-Eval/src/results/pairwise/ --response_order swapped --criteria_dir ./MELD-Eval/src/criteria/
    ```
   Input Format
    ```json
    [
      {
        "id": 1,
        "question_body": "Question text",
        "answer1_body": "First response",
        "answer2_body": "Second response",
        "label": "Human reference label (A, B, or Tie)",  
        "category": "Category" 
      }
    ]
    ```
     Command-line Arguments
    - `--model`: Evaluation model to run (meld, llama3, autoj, prometheus, pandalm, all)
    - `--input_file`: Path to input JSON file
    - `--output_dir`: Directory for output files
    - `--criteria_dir`: Directory containing criteria files (required for MELD)
    - `--*_model_path`: Paths to respective evaluation models
    - `--seed`: Random seed for reproducibility
    
    Additional Pairwise Comparison Parameters
    - `--response_order`: Response evaluation order (original, swapped)
---
3. **Result Evaluation**
    
    - Pointwise grading
    
    ```bash
    python ./MELD-Eval/src/eval/evaluate_pointwise.py --input_dir ./MELD-Eval/src/results/pointwise/ --output_csv ./MELD-Eval/src/results/pointwise/correlation_results.csv
    ```
    - Pariwise comparison
    
    ```bash
   python ./MELD-Eval/src/eval/evaluate_pairwise.py --input_dir ./MELD-Eval/src/results/pairwise/ --output_csv ./MELD-Eval/src/results/pairwise/pairwise_metrics.csv
    ```
---

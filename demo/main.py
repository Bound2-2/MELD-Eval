import streamlit as st
import requests
import json
import re
import plotly.express as px
from rouge_score import rouge_scorer
import sacrebleu
from bert_score import score as bert_score
import pandas as pd
import plotly.graph_objects as go

###ç»˜å›¾###
def compute_rouge(reference, candidate):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, candidate)
    return {key: score.fmeasure for key, score in scores.items()}

def compute_bertscore(reference, candidate):
    P, R, F1 = bert_score([candidate], [reference], lang='en', model_type='bert-base-uncased', verbose=True)
    return F1.mean().item()

def generate_metrics_plot(rouge_scores, bleu_score, bert_score):
    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', 'BERTScore']
    scores = [
        rouge_scores['rouge1'],
        rouge_scores['rouge2'],
        rouge_scores['rougeL'],
        bleu_score,
        bert_score,
    ]
    
    df = pd.DataFrame({
        'Metrics': metrics,
        'Scores': scores
    })
    
    fig = px.bar(df, x='Metrics', y='Scores', text='Scores',
                 color='Scores', color_continuous_scale=px.colors.sequential.Viridis)
    fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
    fig.update_layout(
        uniformtext_minsize=8, 
        uniformtext_mode='hide',
        xaxis_title='',  # å»æ‰xè½´æ ‡é¢˜
        margin=dict(l=0, r=0, t=0, b=0),  # Reduce margins to use more space for the chart
        height=350,
    )
    return fig


def plot_scores_PAIRWISE(processed_response, for_chart):
    multi_dimension_score = {
                        'score_A': processed_response['score_A'],
                        'score_B': processed_response['score_B']
                    }
    metrics = list(multi_dimension_score['score_A'].keys())
    scores_A = [multi_dimension_score['score_A'][metric] for metric in metrics]
    scores_B = [multi_dimension_score['score_B'][metric] for metric in metrics]
    
    # åˆ›å»º Plotly å›¾è¡¨
    fig1 = go.Figure(data=[
        go.Bar(name='Score A', x=metrics, y=scores_A),
        go.Bar(name='Score B', x=metrics, y=scores_B)
    ])
    
    # æ›´æ–°å›¾è¡¨å¸ƒå±€
    fig1.update_layout(
        barmode='group',
        yaxis_title='Scores',
        legend_title='Score Groups',
        margin=dict(l=10, r=10, t=30, b=10),
        height=400
    )

    # åªæœ‰åœ¨referenceä¸ä¸ºç©ºä¸”ä¸æ˜¯"N/A"æ—¶æ‰ç”Ÿæˆå‚è€ƒå¯¹æ¯”å›¾
    if for_chart["reference"] and for_chart["reference"] != "N/A":
        #ç¬¬ä¸€å¼ å’Œreferenceæ¯”è¾ƒå›¾
        reference = for_chart["reference"]
        candidate1 = for_chart["answer1"]
        rouge_scores = compute_rouge(reference, candidate1)
        bleu_score = sacrebleu.corpus_bleu([candidate1], [[reference]]).score
        bert_score_val = compute_bertscore(reference, candidate1)
        fig2 = generate_metrics_plot(rouge_scores, bleu_score/10, bert_score_val)

        #ç¬¬äºŒå¼ å’Œreferenceæ¯”è¾ƒå›¾
        reference = for_chart["reference"]
        candidate1 = for_chart["answer2"]
        rouge_scores = compute_rouge(reference, candidate1)
        bleu_score = sacrebleu.corpus_bleu([candidate1], [[reference]]).score
        bert_score_val = compute_bertscore(reference, candidate1)
        fig3 = generate_metrics_plot(rouge_scores, bleu_score/10, bert_score_val)
        
        return fig1, fig2, fig3
    else:
        # å¦‚æœæ²¡æœ‰å‚è€ƒç­”æ¡ˆï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªå›¾è¡¨
        return fig1, None, None
    

def plot_scores_POINTWISE(processed_response, for_chart):
    # å‡è®¾ processed_response åŒ…å« 'score_A' æˆ–ç±»ä¼¼å­—æ®µ
    dimension_scores = processed_response["Dimension_Scores"]  # ä½¿ç”¨ processed_response ä½œä¸ºè¾“å…¥æ•°æ®
    
    metrics = list(dimension_scores.keys())
    scores = list(dimension_scores.values())

    # åˆ›å»º Plotly å›¾è¡¨
    fig1 = go.Figure(data=[
        go.Bar(x=metrics, y=scores)
    ])

    # æ›´æ–°å›¾è¡¨å¸ƒå±€
    fig1.update_layout(
        yaxis_title='Score',
        xaxis=dict(type='category'),  # ç¡®ä¿xè½´ä¸ºç±»åˆ«ç±»å‹
        yaxis=dict(range=[0, max(scores) + 1]),  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´yè½´èŒƒå›´
        margin=dict(l=10, r=10, t=30, b=10),
        height=400
    )
    
    # åªæœ‰åœ¨referenceä¸ä¸ºç©ºä¸”ä¸æ˜¯"N/A"æ—¶æ‰ç”Ÿæˆå‚è€ƒå¯¹æ¯”å›¾
    if for_chart["reference"] and for_chart["reference"] != "N/A":
        #ç¬¬ä¸€å¼ å’Œreferenceæ¯”è¾ƒå›¾
        reference = for_chart["reference"]
        candidate1 = for_chart["answer"]
        rouge_scores = compute_rouge(reference, candidate1)
        bleu_score = (sacrebleu.corpus_bleu([candidate1], [[reference]]).score)/10
        bert_score_val = compute_bertscore(reference, candidate1)
        fig2 = generate_metrics_plot(rouge_scores, bleu_score, bert_score_val)
        return fig1, fig2
    else:
        # å¦‚æœæ²¡æœ‰å‚è€ƒç­”æ¡ˆï¼Œåªè¿”å›ç¬¬ä¸€ä¸ªå›¾è¡¨
        return fig1, None


# Function to extract required parts from gpt_response
def extract_gpt_response_info_pairwise(gpt_response):
    # æ·»åŠ é”™è¯¯å¤„ç†ï¼Œç¡®ä¿æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç»“æœæœ‰æ•ˆ
    try:
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç”¨äºæå–å„éƒ¨åˆ†
        pattern_a = r"@@@(.*?)@@@"
        pattern_b = r"@@@(.*?)###"
        pattern_final_result = r"###(.*?)&&&"
        pattern_detailed_feedback = r"&&&Detailed Evaluation Feedback:(.*?)\*\*\*"
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
        match_a = re.search(pattern_a, gpt_response, re.DOTALL)
        match_b = re.search(pattern_b, gpt_response, re.DOTALL)
        match_final_result = re.search(pattern_final_result, gpt_response, re.DOTALL)
        match_detailed_feedback = re.search(pattern_detailed_feedback, gpt_response, re.DOTALL)
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result = {}

        # å¯¹ dict_A å’Œ dict_B ä½¿ç”¨å­—ç¬¦ä¸²è§£æï¼ˆéæ ‡å‡†JSONæ ¼å¼æ— æ³•ç›´æ¥è§£æï¼‰
        dict_a_raw = match_a.group(1).strip() if match_a else ""
        dict_b_raw = match_b.group(1).strip() if match_b else ""
        
        # å°†è‡ªå®šä¹‰çš„æ ¼å¼è½¬æ¢ä¸ºé”®å€¼å¯¹å­—å…¸
        def parse_custom_format(raw_text):
            scores = {}
            # åŒ¹é…ç±»ä¼¼ 'Key': value çš„æ ¼å¼
            matches = re.findall(r"'(.*?)':\s*(\d+)", raw_text)
            for key, value in matches:
                scores[key] = int(value)
            return scores
        
        # è§£æ dict_A å’Œ dict_B
        result['score_A'] = parse_custom_format(dict_a_raw)
        result['score_B'] = parse_custom_format(dict_b_raw)
        result['final_results'] = match_final_result.group(1).strip() if match_final_result else "No clear result"
        result['Detailed_Evaluation_Feedback'] = match_detailed_feedback.group(1).strip() if match_detailed_feedback else "No detailed feedback available"
        
        # ç¡®ä¿score_Aå’Œscore_Bè‡³å°‘æœ‰ä¸€ä¸ªé”®å€¼å¯¹
        if not result['score_A']:
            result['score_A'] = {'Overall': 5}
        if not result['score_B']:
            result['score_B'] = {'Overall': 5}
            
        return result
    except Exception as e:
        # å¦‚æœæå–è¿‡ç¨‹å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        st.error(f"Error parsing model response: {str(e)}")
        st.code(gpt_response, language="text")
        return {
            'score_A': {'Overall': 5},
            'score_B': {'Overall': 5},
            'final_results': "Error parsing results",
            'Detailed_Evaluation_Feedback': "Could not extract detailed feedback from model response."
        }

def extract_gpt_response_info_pointwise(gpt_response):
    try:
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ç”¨äºæå–å„éƒ¨åˆ†
        pattern_dict_a = r"@@@Dimension Scores:\s*(\{.*?\})###"
        pattern_dict_b = r"###Overall Score:\s*(\d+)&&&"
        pattern_detailed_feedback = r"&&&Detailed Evaluation Feedback:(.*?)\*\*\*"

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
        match_dict_a = re.search(pattern_dict_a, gpt_response, re.DOTALL)
        match_dict_b = re.search(pattern_dict_b, gpt_response, re.DOTALL)
        match_detailed_feedback = re.search(pattern_detailed_feedback, gpt_response, re.DOTALL)

        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result = {}

        # æ‰‹åŠ¨è§£æè‡ªå®šä¹‰æ ¼å¼çš„å­—å…¸
        def parse_custom_format(raw_text):
            scores = {}
            # åŒ¹é…ç±»ä¼¼ 'Key': value çš„æ ¼å¼ (å…¶ä¸­valueæ˜¯æ•´æ•°)
            matches = re.findall(r"'(.*?)':\s*(\d+)", raw_text)
            for key, value in matches:
                scores[key] = int(value)
            return scores

        # è§£æå­—å…¸A (Dimension Scores)
        dict_a_raw = match_dict_a.group(1).strip() if match_dict_a else ""
        dict_a = parse_custom_format(dict_a_raw)

        # è§£æå­—å…¸B (Overall Score)
        dict_b = {"Overall Score": int(match_dict_b.group(1).strip())} if match_dict_b else {"Overall Score": 5}

        # è§£æè¯¦ç»†åé¦ˆ (Detailed Evaluation Feedback)
        detailed_feedback = match_detailed_feedback.group(1).strip() if match_detailed_feedback else "No detailed feedback available"

        # ç¡®ä¿Dimension_Scoresè‡³å°‘æœ‰ä¸€ä¸ªé”®å€¼å¯¹
        if not dict_a:
            dict_a = {'Overall': 5}
            
        # å°†è§£æçš„å†…å®¹å­˜å…¥ç»“æœå­—å…¸
        result['Dimension_Scores'] = dict_a
        result['Overall_Score'] = dict_b
        result['Detailed_Evaluation_Feedback'] = detailed_feedback

        return result
    except Exception as e:
        # å¦‚æœæå–è¿‡ç¨‹å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        st.error(f"Error parsing model response: {str(e)}")
        st.code(gpt_response, language="text")
        return {
            'Dimension_Scores': {'Overall': 5},
            'Overall_Score': {"Overall Score": 5},
            'Detailed_Evaluation_Feedback': "Could not extract detailed feedback from model response."
        }


def read_criteria(scenario):
    """æ ¹æ®åœºæ™¯è¯»å–ç›¸åº”çš„è¯„ä»·æ ‡å‡†æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(f'/root/autodl-tmp/demo/txt_criteria/{scenario}.txt', 'r', encoding='utf-8') as file:
            criteria = file.read()
        return criteria
    except FileNotFoundError:
        print(f"No criteria found for {scenario}")
        return "No specific criteria available for this scenario."

def user_selected_criteria(criteria_list):
    # éå†åˆ—è¡¨ï¼Œå°†æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºå¸¦æœ‰åºå·çš„æ ¼å¼
    formatted_criteria = [f"{i+1}. {criteria}" for i, criteria in enumerate(criteria_list)]
    # å°†æ‰€æœ‰å…ƒç´ åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå…ƒç´ å ä¸€è¡Œ
    return "\n".join(formatted_criteria)

# æ£€æŸ¥OllamaæœåŠ¡
def check_ollama_service():
    try:
        # å°è¯•ä¸€ä¸ªç®€å•çš„APIè°ƒç”¨
        response = requests.get("http://localhost:6006/api/version", timeout=2)
        return response.status_code == 200
    except:
        return False

# ç›´æ¥è°ƒç”¨Ollama API
def call_ollama_api(model_name, prompt, message_placeholder=None):
    """ç›´æ¥è°ƒç”¨Ollama APIå¹¶å¤„ç†æµå¼å“åº”"""
    try:
        # å‡†å¤‡APIè¯·æ±‚
        headers = {"Content-Type": "application/json"}
        data = {
            "model": model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True  # ä½¿ç”¨æµå¼å“åº”
        }
        
        # å‘é€è¯·æ±‚
        response = requests.post(
            "http://localhost:11434/api/chat",
            headers=headers,
            json=data,
            stream=True,
            timeout=300
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code != 200:
            return f"Error: API returned status code {response.status_code} - {response.text}"
        
        # å¤„ç†æµå¼å“åº”
        full_response = ""
        
        for line in response.iter_lines():
            if not line:
                continue
                
            try:
                # è§£ææ¯è¡ŒJSON
                chunk = json.loads(line)
                
                # æå–å†…å®¹
                if 'message' in chunk and 'content' in chunk['message']:
                    content = chunk['message']['content']
                    if content:
                        full_response += content
                        
                        # å¦‚æœæä¾›äº†å ä½ç¬¦ï¼Œæ›´æ–°UI
                        if message_placeholder:
                            message_placeholder.markdown(full_response, unsafe_allow_html=True)
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if chunk.get('done', False):
                    break
                    
            except json.JSONDecodeError:
                continue
            
        return full_response
        
    except Exception as e:
        error_msg = f"API call error: {str(e)}"
        if message_placeholder:
            message_placeholder.error(error_msg)
        return error_msg

# App title
st.set_page_config(page_title="ğŸ‘¨â€âš–ï¸MELD",layout="wide")

# ä½¿ç”¨session_stateå­˜å‚¨éœ€è¦æŒä¹…åŒ–çš„å˜é‡
if "model_name" not in st.session_state:
    st.session_state["model_name"] = "q4k_meld:latest"

if "messages" not in st.session_state:
    st.session_state.messages = []

if "evaluation_mode" not in st.session_state:
    st.session_state.evaluation_mode = "PAIRWISE"

with st.sidebar:
    st.title('ğŸ‘¨â€âš–ï¸MELD')
    st.write('A Fine-Grained Evaluation Framework for Language Models: Combining Pointwise Grading and Pairwise Comparison.')
    
    # æ·»åŠ è¯„ä¼°æ¨¡å¼é€‰æ‹©
    st.subheader('Evaluation Mode')
    evaluation_mode = st.radio(
        "Select Evaluation Mode",
        ["PAIRWISE", "POINTWISE"],
        index=0 if st.session_state.evaluation_mode == "PAIRWISE" else 1,
        help="PAIRWISE compares two answers. POINTWISE evaluates a single answer."
    )
    st.session_state.evaluation_mode = evaluation_mode
    
    st.subheader('Model and parameters')
    # ä½¿ç”¨å›ºå®šæ¨¡å‹
    st.write('Using model: q4k_meld:latest')
    st.session_state["model_name"] = "q4k_meld:latest"
    st.divider()

    temperature = st.slider('temperature', min_value=0.01, max_value=1.0, value=0.1, step=0.01)
    top_p = st.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.slider('max_length', min_value=1024, max_value=8192, value=1024, step=8)
    
    # åœ¨ä¾§è¾¹æ ä¸­æ·»åŠ æ¨¡å‹éªŒè¯æŒ‰é’®
    if st.button("Test Model Connection"):
        try:
            st.info("Testing connection...")
            test_prompt = "Hello, how are you?"
            test_response = call_ollama_api(st.session_state["model_name"], test_prompt)
            
            if isinstance(test_response, str) and test_response.startswith("Error"):
                st.error(test_response)
            else:
                st.success(f"Connection successful! Response: {test_response[:50]}...")
        except Exception as e:
            st.error(f"Connection failed: {str(e)}")
    
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
    st.session_state.clear()  # æ¸…é™¤ st.session_state ä¸­çš„æ‰€æœ‰å†…å®¹
    st.session_state['question_body'] = ""
    st.session_state['answer1_body'] = ""
    st.session_state['answer2_body'] = ""
    st.session_state['answer_body'] = ""
    st.session_state['reference'] = ""
    st.session_state["model_name"] = "q4k_meld:latest"
    st.session_state.evaluation_mode = "PAIRWISE"
    
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
if not check_ollama_service():
    st.error("âš ï¸ OllamaæœåŠ¡ä¸å¯ç”¨ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ 'ollama serve' å¯åŠ¨æœåŠ¡ååˆ·æ–°æ­¤é¡µé¢ã€‚")
    st.code("ollama serve", language="bash")
    st.stop()

st.title(f"MELD - {st.session_state.evaluation_mode} Evaluation")

if 'file_processed' not in st.session_state:
    st.session_state.file_processed = False

uploaded_file = st.file_uploader(
        "",
        type=["json", "jsonl"],
        help="Scanned documents are not supported yet!",
    )

# åœ¨è¿™è¡Œä»£ç åé¢ç«‹å³æ·»åŠ æ–°æ–‡ä»¶æ£€æµ‹ä»£ç 
if uploaded_file:
    file_name = uploaded_file.name
    
    # åœ¨ä¼šè¯çŠ¶æ€ä¸­è·Ÿè¸ªä¸Šä¼ çš„æ–‡ä»¶å
    if 'previous_file_name' not in st.session_state:
        st.session_state.previous_file_name = ""
    
    # å½“ä¸Šä¼ æ–°æ–‡ä»¶æ—¶é‡ç½®å¤„ç†æ ‡å¿—
    if st.session_state.previous_file_name != file_name:
        st.session_state.file_processed = False
        st.session_state.previous_file_name = file_name
        st.info(f"New file detectedï¼š{file_name}")

if st.session_state.file_processed:
    if st.button("Process another file"):
        st.session_state.file_processed = False
        st.experimental_rerun()


# options = ['default', 'analyzing_general', 'asking_how_to_question', 'brainstorming', 'chitchat', 'classification_identification', 'code_correction_rewriting', 'code_generation', 'code_to_code_translation', 'counterfactual', 'creative_writing', 'data_analysis', 'explaining_code', 'explaining_general', 'functional_writing', 'information_extraction', 'instructional_rewriting', 'keywords_extraction', 'language_polishing', 'math_reasoning', 'open_question', 'paraphrasing', 'planning', 'question_generation', 'ranking', 'reading_comprehension', 'recommendation', 'roleplay', 'seeking_advice', 'solving_exam_question_with_math', 'solving_exam_question_without_math', 'text_correction', 'text_simplification', 'text_summarization', 'text_to_text_translation', 'title_generation', 'topic_modeling', 'value_judgement', 'verifying_fact', 'writing_advertisement', 'writing_cooking_recipe', 'writing_email', 'writing_job_application', 'writing_news_article', 'writing_personal_essay', 'writing_presentation_script', 'writing_product_description', 'writing_social_media_post', 'writing_song_lyrics']
options = ['Writing', 'Math', 'Reasoning', 'NLP Task', 'Coding', 'casual conversation', 'Professional Knowledge', 'Roleplay']

# åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªé€‰æ‹©çš„å˜é‡
if 'selected_option' not in st.session_state:
    st.session_state.selected_option = options[0]

def update_selection(option):
    st.session_state.selected_option = option

# ä½¿ç”¨expanderæ¥ç»„ç»‡æ˜¾ç¤ºï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶æŠ˜å å’Œå±•å¼€
with st.expander("Choose a category"):
    # è®¡ç®—éœ€è¦çš„è¡Œå’Œåˆ—æ•°é‡
    num_rows = len(options) // 4 + (1 if len(options) % 4 > 0 else 0)
    rows = [st.columns(4) for _ in range(num_rows)]
    option_index = 0

    for row in rows:
        for col in row:
            with col:
                # ä»…åœ¨è¿˜æœ‰é€‰é¡¹æ—¶æ˜¾ç¤ºå•é€‰æŒ‰é’®
                if option_index < len(options):
                    # æ£€æŸ¥è¿™ä¸ªé€‰é¡¹æ˜¯å¦è¢«é€‰ä¸­
                    is_checked = st.radio(
                        "", [options[option_index]],
                        key=f"option_{option_index}",  # ç¡®ä¿æ¯ä¸ªå•é€‰æŒ‰é’®ç»„çš„keyä¸åŒ
                        index=0 if st.session_state.selected_option == options[option_index] else None,
                        on_change=update_selection,
                        args=(options[option_index],)
                    )
                    option_index += 1

# æ˜¾ç¤ºé€‰ä¸­çš„é€‰é¡¹
st.write("You selected:", st.session_state.selected_option)

# å®šä¹‰ç»´åº¦è¯„ä¼°å¤é€‰æ¡†çš„é€‰é¡¹
options_group_1 = [
    "User Satisfaction",
    "Accuracy",
    "Information Richness",
    "Text Quality",
    "Logical Coherence ",
    "Creativity",
    "Being Friendly",
    "Vivid",
    "Engagement",
    "Completeness"
]
options_group_2 = [
    "Relevance",
    "Clarity",
    "Code Correctness",
    "Completeness of Code",
    "Code Readability",
    "Input/Output Requirements",
    "Documentation",
    "Modularity",
    "Running Efficiency",
    "Structure"
]
options_group_3 = [
    "Clarity",
    "Relevance to Topic/Text",
    "Depth",
    "Coherence",
    "Originality",
    "Instruction Following",
    "Fluency",
    "Engagement",
    "Detail",
    "Creativity"
]
options_group_4 = [
    "Structure",
    "Conciseness",
    "Correctness (Math)",
    "Step-by-Step Explanation",
    "Depth of Understanding"
]

# ç”¨äºå­˜å‚¨ç”¨æˆ·é€‰æ‹©çš„é€‰é¡¹ï¼Œä½¿ç”¨æ–°çš„å˜é‡å criteria_selected_option
if 'criteria_selected_option' not in st.session_state:
    st.session_state.criteria_selected_option = {
        "group_1": [],
        "group_2": [],
        "group_3": [],
        "group_4": []
    }

# è®¡ç®—æ€»çš„é€‰é¡¹æ•°é‡
total_selected = sum(len(st.session_state.criteria_selected_option[group]) for group in st.session_state.criteria_selected_option)

# è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œè¶…è¿‡ 10 ä¸ªé€‰é¡¹æ—¶ç¦ç”¨å¤é€‰æ¡†
disable_checkboxes = total_selected >= 10

# åˆ›å»ºä¸€ä¸ªåŒ…å« 4 ç»„å¤é€‰æ¡†çš„æ¨ªå‘æ’åˆ—
with st.expander("Select evaluation criteria"):
    # ä½¿ç”¨ st.columns åˆ›å»º 4 åˆ—å¸ƒå±€
    cols = st.columns(4)

    # åœ¨æ¯åˆ—çš„é¡¶éƒ¨æ·»åŠ ç»„åç§°
    with cols[0]:
        st.write("basic standard")
    with cols[1]:
        st.write("style")
    with cols[2]:
        st.write("content")
    with cols[3]:
        st.write("format")

# è·å–æ‰€æœ‰ç»„ä¸­çš„æœ€çŸ­é•¿åº¦
min_length = min(
    len(options_group_1),
    len(options_group_2),
    len(options_group_3),
    len(options_group_4)
)

# åœ¨æ¯ä¸€åˆ—ä¸­æ”¾ç½®å¤é€‰æ¡†
for i in range(min_length):
    # ç¬¬ 1 ç»„å¤é€‰æ¡†
    with cols[0]:
        option = options_group_1[i]
        checked = option in st.session_state.criteria_selected_option["group_1"]
        if st.checkbox(option, key=f"group_1_{option}", value=checked, disabled=not checked and disable_checkboxes):
            if option not in st.session_state.criteria_selected_option["group_1"]:
                st.session_state.criteria_selected_option["group_1"].append(option)
        else:
            if option in st.session_state.criteria_selected_option["group_1"]:
                st.session_state.criteria_selected_option["group_1"].remove(option)

    # ç¬¬ 2 ç»„å¤é€‰æ¡†
    with cols[1]:
        option = options_group_2[i]
        checked = option in st.session_state.criteria_selected_option["group_2"]
        if st.checkbox(option, key=f"group_2_{option}", value=checked, disabled=not checked and disable_checkboxes):
            if option not in st.session_state.criteria_selected_option["group_2"]:
                st.session_state.criteria_selected_option["group_2"].append(option)
        else:
            if option in st.session_state.criteria_selected_option["group_2"]:
                st.session_state.criteria_selected_option["group_2"].remove(option)

    # ç¬¬ 3 ç»„å¤é€‰æ¡†
    with cols[2]:
        option = options_group_3[i]
        checked = option in st.session_state.criteria_selected_option["group_3"]
        if st.checkbox(option, key=f"group_3_{option}", value=checked, disabled=not checked and disable_checkboxes):
            if option not in st.session_state.criteria_selected_option["group_3"]:
                st.session_state.criteria_selected_option["group_3"].append(option)
        else:
            if option in st.session_state.criteria_selected_option["group_3"]:
                st.session_state.criteria_selected_option["group_3"].remove(option)

    # ç¬¬ 4 ç»„å¤é€‰æ¡†
    with cols[3]:
        option = options_group_4[i]
        checked = option in st.session_state.criteria_selected_option["group_4"]
        if st.checkbox(option, key=f"group_4_{option}", value=checked, disabled=not checked and disable_checkboxes):
            if option not in st.session_state.criteria_selected_option["group_4"]:
                st.session_state.criteria_selected_option["group_4"].append(option)
        else:
            if option in st.session_state.criteria_selected_option["group_4"]:
                st.session_state.criteria_selected_option["group_4"].remove(option)

# è¾“å‡ºé€‰ä¸­çš„åç§°
selected_criteria = []
for group in st.session_state.criteria_selected_option:
    selected_criteria.extend(st.session_state.criteria_selected_option[group])

# æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„æ ‡å‡†
if 0 < len(selected_criteria) < 5:
    st.warning("You must select either 0 or at least 5 criteria.")
    disable_other_operations = True
    st.stop() 
else:
    disable_other_operations = False

# åªæœ‰å½“é€‰ä¸­ 0 ä¸ªæˆ– 5 ä¸ªåŠä»¥ä¸Šé€‰é¡¹æ—¶ï¼Œæ‰å…è®¸æ‰§è¡Œå…¶ä»–æ“ä½œ
if not disable_other_operations:
    # ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œ
    if selected_criteria:
        st.write(f"You selected: {', '.join(selected_criteria)}")  # è¾“å‡ºæ ¼å¼ä¸º "You selected: ..."
    else:
        st.write("You selected: No criteria selected.")

# PAIRWISEæ¨¡å¼
if st.session_state.evaluation_mode == "PAIRWISE":
    # æ–‡ä»¶ä¸Šä¼ å¤„ç†éƒ¨åˆ†
    if uploaded_file and not st.session_state.file_processed:
        try:
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹
            uploaded_file.seek(0)
            file_content = uploaded_file.read().decode('utf-8')  # è¯»å–å¹¶è§£ç ä¸ºUTF-8æ ¼å¼çš„å­—ç¬¦ä¸²
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            st.write("File content sample (first 200 characters):")
            st.write(file_content[:200])
            
            # å°è¯•è§£æJSONæ•°æ®
            try:
                data = json.loads(file_content)  # è§£æJSONæ•°æ®
            except json.JSONDecodeError as json_err:
                st.error(f"JSON parsing error: {str(json_err)}")
                st.error("Please upload a valid JSON file.")
                st.stop()
            
            # ç¡®ä¿æ•°æ®æ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(data, list):
                st.error("The uploaded JSON must be in array format.")
                st.write(f"Current data type: {type(data)}")
                st.stop()
            
            if len(data) == 0:
                st.error("The uploaded JSON array is empty.")
                st.stop()
            
            # æ£€æŸ¥æ¯ä¸ªå…ƒç´ æ˜¯å¦åŒ…å«å¿…è¦çš„é”®
            missing_keys_items = []
            for i, item in enumerate(data):
                if not isinstance(item, dict):
                    st.error(f"Item {i+1} in the array is not a dictionary.")
                    st.stop()
                
                # ä¸¥æ ¼æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
                required_keys = {'question_body', 'answer1_body', 'answer2_body'}
                if not all(key in item for key in required_keys):
                    missing_keys = required_keys - set(item.keys())
                    missing_keys_items.append((i, missing_keys))
            
            # å¦‚æœæœ‰ä»»ä½•é¡¹ç›®ç¼ºå°‘å¿…è¦çš„é”®ï¼Œæ˜¾ç¤ºé”™è¯¯å¹¶åœæ­¢
            if missing_keys_items:
                st.error("The following items are missing required fields:")
                for i, missing in missing_keys_items:
                    st.write(f"Item {i+1}: Missing {', '.join(missing)}")
                st.error("Please ensure all items include 'question_body', 'answer1_body', and 'answer2_body' fields.")
                st.stop()
            
            # æ•°æ®éªŒè¯é€šè¿‡ï¼Œç»§ç»­å¤„ç†
            st.success(f"Successfully validated {len(data)} items, all items contain the required fields.")
            
            # è¾“å‡ºæ–‡ä»¶è·¯å¾„
            modified_file_path = 'critic_by_pairwise_data.json'
            
            # å¤„ç†ä¸Šä¼ çš„æ•°æ®
            processed_items = []
            
            # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ä½¿ç”¨ä¸€ä¸ªå®¹å™¨æ¥æ˜¾ç¤ºå¤„ç†ç»“æœ
            results_container = st.container()
            
            for i, item in enumerate(data):
                # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress = (i) / len(data)
                progress_bar.progress(progress)
                status_text.text(f"Processing item {i+1}/{len(data)} ({int(progress*100)}%)")
                
                try:
                    with results_container.expander(f"Item {i+1}/{len(data)}", expanded=(i==0)):
                        st.write(f"Question: {item['question_body'][:100]}...")
                        
                        # è·å–å¿…é¡»çš„é”®
                        question_body = item['question_body']
                        answer1_body = item['answer1_body']
                        answer2_body = item['answer2_body']
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å‚è€ƒç­”æ¡ˆ
                        reference = item.get('reference', "")
                        
                        # é€‰æ‹©é€‚å½“çš„æ¨¡æ¿æ–‡ä»¶
                        if reference:
                            template_file = "/root/autodl-tmp/demo/prompt_template/PAIRWISE_WR.txt"
                        else:
                            template_file = "/root/autodl-tmp/demo/prompt_template/PAIRWISE_WOR.txt"
                        
                        # å°è¯•è¯»å–æ¨¡æ¿æ–‡ä»¶
                        try:
                            with open(template_file, "r") as file:
                                base_prompt = file.read()
                        except FileNotFoundError:
                            st.error(f"Template file {template_file} not found.")
                            # å°è¯•å¤‡ç”¨è·¯å¾„
                            backup_paths = [
                                f"./prompt_template/{os.path.basename(template_file)}",
                                f"../prompt_template/{os.path.basename(template_file)}",
                                f"prompt_template/{os.path.basename(template_file)}"
                            ]
                            template_found = False
                            for path in backup_paths:
                                try:
                                    with open(path, "r") as file:
                                        base_prompt = file.read()
                                        template_found = True
                                        st.success(f"Using backup template: {path}")
                                        break
                                except FileNotFoundError:
                                    continue
                            
                            if not template_found:
                                st.error("Could not find the necessary template file.")
                                continue  # Skip this item and move to the next one
                        
                        # ç¡®å®šè¯„ä»·ç±»åˆ«
                        scenario = item.get('category', st.session_state.selected_option)
                        
                        # ç”Ÿæˆæœ€ç»ˆçš„prompt
                        if not selected_criteria:
                            final_prompt = base_prompt.format(
                                scenario = scenario,
                                criteria = read_criteria(scenario),
                                question_body = question_body,
                                answer1_body = answer1_body,
                                answer2_body = answer2_body,
                                reference = reference if reference else "N/A"
                            )
                        else:
                            final_prompt = base_prompt.format(
                                scenario = scenario,
                                criteria = user_selected_criteria(selected_criteria),
                                question_body = question_body,
                                answer1_body = answer1_body,
                                answer2_body = answer2_body,
                                reference = reference if reference else "N/A"
                            )
                        
                        # ä½¿ç”¨å ä½ç¬¦æ˜¾ç¤ºåŠ è½½çŠ¶æ€å’Œå“åº”
                        message_placeholder = st.empty()
                        
                        try:
                            with st.spinner(f'Evaluating answers...'):
                                # ä½¿ç”¨APIè°ƒç”¨è·å–å“åº”
                                full_response = call_ollama_api(
                                    st.session_state["model_name"], 
                                    final_prompt, 
                                    message_placeholder
                                )
                        except Exception as e:
                            st.error(f"Error connecting to Ollama: {str(e)}")
                            st.error("Please ensure Ollama service is running.")
                            item['processing_error'] = f"Connection error: {str(e)}"
                            processed_items.append(item)
                            continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªé¡¹ç›®
                        
                        message_placeholder.empty()
                        
                        try:
                            # å¤„ç†å“åº”
                            processed_response = extract_gpt_response_info_pairwise(full_response)
                            
                            final_result = str(processed_response["final_results"]).replace("Final Result: ", "")
                            result_text = "ğŸ¤ It's a Tie!" if final_result == "Tie" else f"ğŸ† {final_result} Wins!"
                            
                            # æ˜¾ç¤ºç»“æœ
                            col1, col2 = st.columns([1, 3])
                            
                            # æ ·å¼è®¾ç½®
                            common_style = "padding: 20px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
                            background_color1 = "background-color: #f8f9fa;"
                            background_color2 = "background-color: #e9ecef;"
                            
                            with col1:
                                st.markdown(f"""
                                    <div style="{background_color1} {common_style}">
                                        <h2 style="color: #007BFF;">{result_text}</h2>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            with col2:
                                st.markdown(f"""
                                    <div style="{background_color2} {common_style}">
                                        <h3 style="color: #6c757d;">Detailed Evaluation Feedback</h3>
                                        <p style="font-size: 16px; line-height: 1.6;">
                                            {processed_response["Detailed_Evaluation_Feedback"]}
                                        </p>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            st.markdown("<hr style='margin: 10px 0;'>", unsafe_allow_html=True)
                            
                            # åªå±•ç¤ºç®€å•çš„åˆ†æ•°å¯¹æ¯”æŸ±çŠ¶å›¾
                            try:
                                multi_dimension_score = {
                                    'score_A': processed_response['score_A'],
                                    'score_B': processed_response['score_B']
                                }
                                metrics = list(multi_dimension_score['score_A'].keys())
                                scores_A = [multi_dimension_score['score_A'][metric] for metric in metrics]
                                scores_B = [multi_dimension_score['score_B'][metric] for metric in metrics]
                                
                                # åˆ›å»º Plotly å›¾è¡¨
                                fig = go.Figure(data=[
                                    go.Bar(name='Answer A', x=metrics, y=scores_A),
                                    go.Bar(name='Answer B', x=metrics, y=scores_B)
                                ])
                                
                                # æ›´æ–°å›¾è¡¨å¸ƒå±€
                                fig.update_layout(
                                    barmode='group',
                                    yaxis_title='Scores',
                                    legend_title='Answers',
                                    margin=dict(l=10, r=10, t=30, b=10),
                                    height=400
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                            except Exception as chart_error:
                                st.error(f"Error creating score comparison chart: {str(chart_error)}")
                            
                            # å°†ç»“æœæ·»åŠ åˆ°åŸæ•°æ®ä¸­
                            item['model_critic'] = full_response
                            item['final_result'] = final_result
                            item['detailed_feedback'] = processed_response["Detailed_Evaluation_Feedback"]
                            item['score_A'] = processed_response.get('score_A', {})
                            item['score_B'] = processed_response.get('score_B', {})
                            
                            processed_items.append(item)
                            st.success(f"Successfully processed item {i+1}")
                            
                        except Exception as e:
                            st.error(f"Error processing item {i+1}: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc(), language="python")
                            
                            # ä¿å­˜éƒ¨åˆ†ç»“æœ
                            item['model_critic'] = full_response if 'full_response' in locals() else "Error during processing"
                            item['processing_error'] = str(e)
                            processed_items.append(item)
                
                except Exception as item_error:
                    st.error(f"Unexpected error processing item {i+1}: {str(item_error)}")
                    import traceback
                    st.code(traceback.format_exc(), language="python")
                    
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                    item['processing_error'] = f"Processing error: {str(item_error)}"
                    processed_items.append(item)
            
            # æ›´æ–°è¿›åº¦æ¡ä¸ºå®Œæˆ
            progress_bar.progress(1.0)
            status_text.text(f"Processed all {len(data)} items")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            try:
                with open(modified_file_path, 'w', encoding='utf-8') as json_file:
                    json.dump(processed_items, json_file, indent=4, ensure_ascii=False)
                st.success(f"All {len(processed_items)} items have been processed and saved to {modified_file_path}")
                
                # è®¾ç½®å¤„ç†å®Œæˆæ ‡å¿—
                st.session_state.file_processed = True
                
            except Exception as save_error:
                st.error(f"Error saving the processed results: {str(save_error)}")
                import traceback
                st.code(traceback.format_exc(), language="python")
        
        except Exception as e:
            st.error(f"Error processing uploaded file: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")

    # æ–‡ä»¶å¤„ç†å®Œæˆåï¼Œæä¾›ä¸‹è½½æŒ‰é’®
    if st.session_state.file_processed:
        try:
            # æä¾›æ–‡ä»¶ä¸‹è½½
            with open('critic_by_pairwise_data.json', 'rb') as f:
                download_clicked = st.download_button(
                    'Download Evaluation JSON File', 
                    f, 
                    file_name='critic_by_pairwise_data.json'
                )
                if download_clicked:
                    st.success("File downloaded successfully!")
                    # é‡ç½®æ ‡å¿—ä»¥å…è®¸å¤„ç†æ–°æ–‡ä»¶
                    st.session_state.file_processed = False
                    st.rerun()  # ä½¿ç”¨ st.rerun() æ›¿ä»£ st.experimental_rerun()
        except Exception as e:
            st.error(f"Error providing download: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")


# POINTWISEæ¨¡å¼
else:
    # æ–‡ä»¶ä¸Šä¼ å¤„ç†éƒ¨åˆ†
    if uploaded_file and not st.session_state.file_processed:
        try:
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹
            uploaded_file.seek(0)
            file_content = uploaded_file.read().decode('utf-8')  # è¯»å–å¹¶è§£ç ä¸ºUTF-8æ ¼å¼çš„å­—ç¬¦ä¸²
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            st.write("File content sample (first 200 characters):")
            st.write(file_content[:200])
            
            # å°è¯•è§£æJSONæ•°æ®
            try:
                data = json.loads(file_content)  # è§£æJSONæ•°æ®
            except json.JSONDecodeError as json_err:
                st.error(f"JSON parsing error: {str(json_err)}")
                st.error("Please upload a valid JSON file.")
                st.stop()
            
            # ç¡®ä¿æ•°æ®æ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(data, list):
                st.error("The uploaded JSON must be in array format.")
                st.write(f"Current data type: {type(data)}")
                st.stop()
            
            if len(data) == 0:
                st.error("The uploaded JSON array is empty.")
                st.stop()
            
            # æ£€æŸ¥æ¯ä¸ªå…ƒç´ æ˜¯å¦åŒ…å«å¿…è¦çš„é”®
            missing_keys_items = []
            for i, item in enumerate(data):
                if not isinstance(item, dict):
                    st.error(f"Item {i+1} in the array is not a dictionary.")
                    st.stop()
                
                # ä¸¥æ ¼æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
                required_keys = {'question_body', 'answer_body'}
                if not all(key in item for key in required_keys):
                    missing_keys = required_keys - set(item.keys())
                    missing_keys_items.append((i, missing_keys))
            
            # å¦‚æœæœ‰ä»»ä½•é¡¹ç›®ç¼ºå°‘å¿…è¦çš„é”®ï¼Œæ˜¾ç¤ºé”™è¯¯å¹¶åœæ­¢
            if missing_keys_items:
                st.error("The following items are missing required fields:")
                for i, missing in missing_keys_items:
                    st.write(f"Item {i+1}: Missing {', '.join(missing)}")
                st.error("Please ensure all items include 'question_body' and 'answer_body' fields.")
                st.stop()
            
            # æ•°æ®éªŒè¯é€šè¿‡ï¼Œç»§ç»­å¤„ç†
            st.success(f"Successfully validated {len(data)} items, all items contain the required fields.")
            
            # è¾“å‡ºæ–‡ä»¶è·¯å¾„
            modified_file_path = 'critic_by_pointwise_data.json'
            
            # å¤„ç†ä¸Šä¼ çš„æ•°æ®
            processed_items = []
            
            # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ä½¿ç”¨ä¸€ä¸ªå®¹å™¨æ¥æ˜¾ç¤ºå¤„ç†ç»“æœ
            results_container = st.container()
            
            for i, item in enumerate(data):
                # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress = (i) / len(data)
                progress_bar.progress(progress)
                status_text.text(f"Processing item {i+1}/{len(data)} ({int(progress*100)}%)")
                
                try:
                    with results_container.expander(f"Item {i+1}/{len(data)}", expanded=(i==0)):
                        st.write(f"Question: {item['question_body'][:100]}...")
                        
                        # è·å–å¿…é¡»çš„é”®
                        question_body = item['question_body']
                        answer_body = item['answer_body']
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å‚è€ƒç­”æ¡ˆ
                        reference = item.get('reference', "")
                        
                        # é€‰æ‹©é€‚å½“çš„æ¨¡æ¿æ–‡ä»¶
                        if reference:
                            template_file = "/root/autodl-tmp/demo/prompt_template/POINTWISE_WR.txt"
                        else:
                            template_file = "/root/autodl-tmp/demo/prompt_template/POINTWISE_WOR.txt"
                        
                        # å°è¯•è¯»å–æ¨¡æ¿æ–‡ä»¶
                        try:
                            with open(template_file, "r") as file:
                                base_prompt = file.read()
                        except FileNotFoundError:
                            st.error(f"Template file {template_file} not found.")
                            # å°è¯•å¤‡ç”¨è·¯å¾„
                            backup_paths = [
                                f"./prompt_template/{os.path.basename(template_file)}",
                                f"../prompt_template/{os.path.basename(template_file)}",
                                f"prompt_template/{os.path.basename(template_file)}"
                            ]
                            template_found = False
                            for path in backup_paths:
                                try:
                                    with open(path, "r") as file:
                                        base_prompt = file.read()
                                        template_found = True
                                        st.success(f"Using backup template: {path}")
                                        break
                                except FileNotFoundError:
                                    continue
                            
                            if not template_found:
                                st.error("Could not find the necessary template file.")
                                continue  # Skip this item and move to the next one
                        
                        # ç¡®å®šè¯„ä»·ç±»åˆ«
                        scenario = item.get('category', st.session_state.selected_option)
                        
                        # ç”Ÿæˆæœ€ç»ˆçš„prompt
                        if not selected_criteria:
                            final_prompt = base_prompt.format(
                                scenario = scenario,
                                criteria = read_criteria(scenario),
                                question_body = question_body,
                                answer_body = answer_body,
                                reference = reference if reference else "N/A"
                            )
                        else:
                            final_prompt = base_prompt.format(
                                scenario = scenario,
                                criteria = user_selected_criteria(selected_criteria),
                                question_body = question_body,
                                answer_body = answer_body,
                                reference = reference if reference else "N/A"
                            )
                        
                        # ä½¿ç”¨å ä½ç¬¦æ˜¾ç¤ºåŠ è½½çŠ¶æ€å’Œå“åº”
                        message_placeholder = st.empty()
                        
                        try:
                            with st.spinner(f'Evaluating answer...'):
                                # ä½¿ç”¨APIè°ƒç”¨è·å–å“åº”
                                full_response = call_ollama_api(
                                    st.session_state["model_name"], 
                                    final_prompt, 
                                    message_placeholder
                                )
                        except Exception as e:
                            st.error(f"Error connecting to Ollama: {str(e)}")
                            st.error("Please ensure Ollama service is running.")
                            item['processing_error'] = f"Connection error: {str(e)}"
                            processed_items.append(item)
                            continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªé¡¹ç›®
                        
                        message_placeholder.empty()
                        
                        try:
                            # å¤„ç†å“åº”
                            processed_response = extract_gpt_response_info_pointwise(full_response)
                            
                            overall_score = processed_response["Overall_Score"]["Overall Score"]
                            result_text = f'ğŸ“ Final Score: <span style="color: #FF4500;">{overall_score}/10</span>'
                            
                            # æ˜¾ç¤ºç»“æœ
                            col1, col2 = st.columns([1, 3])
                            
                            # æ ·å¼è®¾ç½®
                            common_style = "padding: 20px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
                            background_color1 = "background-color: #f8f9fa;"
                            background_color2 = "background-color: #e9ecef;"
                            
                            with col1:
                                st.markdown(f"""
                                    <div style="{background_color1} {common_style}">
                                        <h2 style="color: #007BFF;">{result_text}</h2>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            with col2:
                                st.markdown(f"""
                                    <div style="{background_color2} {common_style}">
                                        <h3 style="color: #6c757d;">Detailed Evaluation Feedback</h3>
                                        <p style="font-size: 16px; line-height: 1.6;">
                                            {processed_response["Detailed_Evaluation_Feedback"]}
                                        </p>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            st.markdown("<hr style='margin: 10px 0;'>", unsafe_allow_html=True)
                            
                            # åªå±•ç¤ºç®€å•çš„ç»´åº¦åˆ†æ•°å›¾è¡¨
                            try:
                                dimension_scores = processed_response["Dimension_Scores"]
                                metrics = list(dimension_scores.keys())
                                scores = list(dimension_scores.values())
                                
                                # åˆ›å»º Plotly å›¾è¡¨
                                fig = go.Figure(data=[
                                    go.Bar(x=metrics, y=scores)
                                ])
                                
                                # æ›´æ–°å›¾è¡¨å¸ƒå±€
                                fig.update_layout(
                                    yaxis_title='Score',
                                    xaxis=dict(type='category'),
                                    yaxis=dict(range=[0, max(scores) + 1]),
                                    margin=dict(l=10, r=10, t=30, b=10),
                                    height=400
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                            except Exception as chart_error:
                                st.error(f"Error creating dimension score chart: {str(chart_error)}")
                            
                            # å°†ç»“æœæ·»åŠ åˆ°åŸæ•°æ®ä¸­
                            item['model_critic'] = full_response
                            item['overall_score'] = overall_score
                            item['dimension_scores'] = processed_response["Dimension_Scores"]
                            item['detailed_feedback'] = processed_response["Detailed_Evaluation_Feedback"]
                            
                            processed_items.append(item)
                            st.success(f"Successfully processed item {i+1}")
                            
                        except Exception as e:
                            st.error(f"Error processing item {i+1}: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc(), language="python")
                            
                            # ä¿å­˜éƒ¨åˆ†ç»“æœ
                            item['model_critic'] = full_response if 'full_response' in locals() else "Error during processing"
                            item['processing_error'] = str(e)
                            processed_items.append(item)
                
                except Exception as item_error:
                    st.error(f"Unexpected error processing item {i+1}: {str(item_error)}")
                    import traceback
                    st.code(traceback.format_exc(), language="python")
                    
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                    item['processing_error'] = f"Processing error: {str(item_error)}"
                    processed_items.append(item)
            
            # æ›´æ–°è¿›åº¦æ¡ä¸ºå®Œæˆ
            progress_bar.progress(1.0)
            status_text.text(f"Processed all {len(data)} items")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            try:
                with open(modified_file_path, 'w', encoding='utf-8') as json_file:
                    json.dump(processed_items, json_file, indent=4, ensure_ascii=False)
                st.success(f"All {len(processed_items)} items have been processed and saved to {modified_file_path}")
                
                # è®¾ç½®å¤„ç†å®Œæˆæ ‡å¿—
                st.session_state.file_processed = True
                
            except Exception as save_error:
                st.error(f"Error saving the processed results: {str(save_error)}")
                import traceback
                st.code(traceback.format_exc(), language="python")
        
        except Exception as e:
            st.error(f"Error processing uploaded file: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")

    # æ–‡ä»¶å¤„ç†å®Œæˆåï¼Œæä¾›ä¸‹è½½æŒ‰é’®
    if st.session_state.file_processed:
        try:
            # æä¾›æ–‡ä»¶ä¸‹è½½
            with open('critic_by_pointwise_data.json', 'rb') as f:
                download_clicked = st.download_button(
                    'Download Evaluation JSON File', 
                    f, 
                    file_name='critic_by_pointwise_data.json'
                )
                if download_clicked:
                    st.success("File downloaded successfully!")
                    # é‡ç½®æ ‡å¿—ä»¥å…è®¸å¤„ç†æ–°æ–‡ä»¶
                    st.session_state.file_processed = False
                    st.rerun()  # ä½¿ç”¨ st.rerun() æ›¿ä»£ st.experimental_rerun()
        except Exception as e:
            st.error(f"Error providing download: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")